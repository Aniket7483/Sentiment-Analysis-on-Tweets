# -*- coding: utf-8 -*-
"""Tweets.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fcQW5ggI7KTyLI283j9aedt5N_1A1nZd
"""

import pandas as pd

# Try reading the file with a different encoding, such as 'latin-1'
df = pd.read_csv('/content/Corona_NLP_test.csv')
df1 = pd.read_csv('/content/Corona_NLP_train.csv', encoding='latin-1')

# If you are unsure of the correct encoding, try using the 'chardet' library to detect it:
import chardet

with open('/content/Corona_NLP_train.csv', 'rb') as f:
    result = chardet.detect(f.read())

df1 = pd.read_csv('/content/Corona_NLP_train.csv', encoding=result['encoding'])

df

df1

import numpy as np
!pip install clean-text
import nltk # Natural Language Tool Kit
from nltk.tokenize import sent_tokenize,word_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.corpus import stopwords
# PortStemmer - for stemming
# WordNetLematizer - for lemmatization
import re # for regular expression - to clean data i.e. to remove special characters, emojis
from cleantext import clean

tweet = df['OriginalTweet']

tweet

tweet2=df1['OriginalTweet']

tweet2

import spacy # contains model which we are going to use
from spacy import displacy # to display along with tag
import pandas as pd

nlp = spacy.load('en_core_web_sm')

nlp.pipe_names

pd.set_option('display.max_colwidth',None)

df

txt = ' '.join(tweet) # with space join all sentences in x
txt

txt1 = ' '.join(tweet2) # with space join all sentences in x
txt1

x = nlp(txt)

x

displacy.render(x,style='ent') # 70 tp 80% accurate model

#!pip install WordCloud
from wordcloud import WordCloud,STOPWORDS
import matplotlib.pyplot as plt

wc = WordCloud(background_color='White',width=800,height=400,stopwords=STOPWORDS).generate(txt)
plt.imshow(wc)#image show
plt.axis('off')
plt.title("WordCloud",size=25)
# bigger the name of word more the frequency of that word in that corpus
# font of apple, laptop is huge i.e. it is occuring many times in text
# word cloud is important when we are analysing the data when we build Naive bias text classification.
# This is how we can create word cloud

